---
title: Attention Is All You Need ! 
tags:
  - 深度学习
  - 自注意力机制
  - 机器翻译
categories: 论文笔记
date: 2018-12-05 16:29:28
cover_img: /images/0bfc8.jpg
feature_img: /images/0bfc8.jpg
description: 由Google Brain在2017年发布的*Attention Is All You Need*使得自注意力机制走进大众视野，其应用逐渐由机器翻译领域，拓展到了自然语言处理中的各项技术。由于我在深度学习、关系抽取等反面的研究需求，对该论文进行了详细研究。本文作为阅读笔记，对其进行了详细梳理，希望能为大家对其理解提供帮助。
---

### 前言
由Google Brain在2017年发布的*Attention Is All You Need*使得自注意力机制走进大众视野，其应用逐渐由机器翻译领域，拓展到了自然语言处理中的各项技术。由于我在深度学习、关系抽取等反面的研究需求，对该论文进行了详细研究。本文作为阅读笔记，对其进行了详细梳理，希望能为大家对其理解提供帮助。

![](/images/Attention Is All You Need !/1.jpg)

### 摘要

现存主要的序列转化模型都是基于复杂的循环神经网络或者卷积神经网络，它们都包含一个编码器和一个解码器。目前效果较好的模型，都是通过注意力机制来对编码器和解码器进行连接。我们提出一种新的简单网络架构Transformer，它仅仅基于注意力机制，完全抛弃了循环和卷积神经网络。两个机器翻译任务的实验表明，我们的模型在质量上更优越，同时更易于并行化，训练花费时间更短。在WMT 2014英语 - 德语翻译任务中达到28.4 BLEU，超过现有的最佳成绩，整体超过2 BLEU。在WMT 2014英语-法语翻译任务中，我们的模型在8个GPU上训练3.5天后，得到了一个新的单模型最高BLEU分数41.0。

### 引言

循环神经网络模型通常沿输入和输出序列的符号位置来进行计算。将位置与计算时的步骤对齐，产生隐藏状态ht序列，作为先前隐藏状态ht-1和位置t的输入的函数。这种固有的序列性质排除了样本训练并行化的可能，这在较长的序列中变得更为严重，因为内存约束限制了跨样本的批处理。最近的有研究通过分解技巧和条件计算实现了计算效率的显著提高，同时后者也提高了模型性能。然而，序列计算的基本约束仍然存在。

注意力机制已成为多种任务序列建模和转化模型中引人注目的组成部分，它允许对依赖关系进行建模，而不用考虑它们在输入或输出序列中的距离。然而，多数情况下这种注意力机制与循环神经网络结合使用。

在这项工作中，我们提出了Transformer，没有循环，而是完全依赖于注意力机制来描述输入和输出之间的全局依赖关系。Transformer允许更多的并行计算，并且在8个P100 GPU上训练了长达12小时后，可以达到一个新的最高翻译水平。

### 背景

减少序列计算的目标构成了Extended Neural GPU、ByteNet和ConvS2S的基础，它们都是将卷积神经网络作为基础模块，并行计算所有输入和输出位置的隐藏表达。在这些模型中，关联任意输入和输出位置符号的运算数量会随着距离的增加而增加，对于ConvS2S呈线性增长，对于ByteNet呈对数增长，这使得学习远距离之间的依赖性变得更加困难。在Transformer中，通过多头注意力将该运算数量被减少到了一个常量，尽管由于位置的平均注意加权而导致有效分辨率有所降低。

自注意力，有时也称为内部注意力，是为计算序列表达关联单序列不同位置的注意力机制。自注意力机制已经被成功应用于阅读理解、文本摘要、文本推理以及学习与任务无关的句子表征等。

端到端的记忆网络是基于循环attention机制，而不是序列对齐的循环，并且已被证明在简单语言问答和语言建模任务中表现良好。

然而，据我们所知，Transformer是第一个用来计算输入和输出表征的完全依赖于自注意力机制的转化模型，该模型中无需使用序列对齐的RNN或CNN。

### 模型架构

这里，编码器映射一个用符号表示的输入序列(x1,...,xn) 到一个连续的表示z = (z1,...,zn)。根据z，解码器生成符号的一个输出序列(y1,...,ym) ，一次一个元素。在每一步中，模型都是自回归的，当生成下一个时，消耗先前生成的符号作为附加输入。

Transformer遵循这种整体架构，编码器和解码器都使用self-attention堆叠和point-wise、完全连接的层，分别显示在图中的左半部分和右半部分。

![](/images/Attention Is All You Need !/2.jpg)

#### 编码器和解码器

编码器：编码器由N = 6 个完全相同的层堆叠而成。每一层都有两个子层。第一层是一个多头自注意力机制，第二层是一个简单的、位置完全连接的前馈网络。我们对每个子层再采用一个残差连接，接着进行层标准化。也就是说，每个子层的输出是LayerNorm(x + Sublayer(x))，其中Sublayer(x) 是由子层本身实现的函数。为了方便这些残差连接，模型中的所有子层以及嵌入层产生的输出维度都为dmodel = 512。

解码器：解码器同样由N = 6 个完全相同的层堆叠而成。除了每个编码器层中的两个子层之外，解码器还插入第三个子层，该层对编码器堆栈的输出执行多头自注意力。与编码器类似，我们在每个子层再采用残差连接，然后进行层标准化。我们还修改解码器堆栈中的自注意力子层，以防止位置关注到后面的位置。这种掩码结合将输出嵌入偏移一个位置，确保对位置 i 的预测只能依赖位置小于 i 的已知输出。

#### 注意力机制

Attention函数可以描述为将query和一组key-value对映射到输出，其中query、key、value和输出都是向量。输出是value的加权和，其中分配给每个value的权重通过query与相应key的兼容函数来计算。

##### 缩放点积注意力

我们称我们的特殊版的Attention为“Scaled Dot-Product Attention”。输入由query、dk 维的key和dv 维的value组成。我们计算query和所有key的点积、并分别用√dk相除，然后应用一个softmax函数以获得值的权重。

![](/images/Attention Is All You Need !/3.png)

在实践中，我们同时计算一组query的attention函数，并将它们组合成一个矩阵Q。 key和value也一起打包成矩阵 K 和 V 。我们计算输出矩阵为：

![](/images/Attention Is All You Need !/4.jpg)

两个最常用的attention函数是加法attention和点积attention。除了缩放因子1/ √dk之外，点积attention与我们的算法相同。加法attention使用具有单个隐藏层的前馈网络计算兼容性函数。虽然两者在理论上的复杂性相似，但在实践中点积attention的速度更快、更节省空间，因为它可以使用高度优化的矩阵乘法代码来实现。

当dk的值比较小的时候，这两个机制的性能相近，当dk比较大时，加法attention比不带缩放的点积attention性能好。我们怀疑，对于很大的dk值，点积大幅度增长，将softmax函数推向具有极小梯度的区域。为了抵消这种影响，我们缩小点积1/√dk倍。

##### 多头注意力

我们发现将query、key和value分别用不同的、学到的线性投影，投影h倍到dk、dk和dv维效果更好，而不是用dmodel维的query、key和value执行单个attention函数。基于每个投影版本的query、key和value，我们并行执行attention函数，产生dv 维输出值。将它们连接并再次映射，产生最终值。

多头注意力机制允许模型联合关注不同位置的不同表示子空间信息。如果只有一个attention head，它的平均值会削弱这个信息。

![](/images/Attention Is All You Need !/5.jpg)

我们采用h = 8 个并行attention层或head。对每个head，我们使用dk =dv =dmodel ∕ h = 64。由于每个head的大小减小，总的计算成本与具有全部维度的单个head attention相似。

##### 本模型中的注意力

Transformer中通过以下三种不同的方式来使用多头注意力：

- 在“encoder-decoder attention”层，query来自前一个解码器层，key和value来自编码器的输出，这允许解码器中的每个位置能关注到输入序列中的所有位置。这模仿序列到序列模型中典型的编码器-解码器的attention机制。

- 编码器包含self-attention层。在self-attention层中，所有的key、value和query来自同一个地方，在这里就是来自于编码器中前一层的输出。编码器中的每个位置都可以关注编码器上一层的所有位置。

- 类似地，解码器中的self-attention层允许解码器中的每个位置都关注到解码器中的所有位置并包括该位置。我们需要防止解码器中的向左信息流来保持自回归属性。通过屏蔽softmax的输入中所有不合法连接的值（设置为-∞），我们在缩放点积attention中实现。

#### 基于位置的前馈网络

除了attention子层之外，我们的编码器和解码器中的每个层都包含一个全连接的前馈网络，该前馈网络单独且相同地应用于每个位置。它由两个线性变换组成，之间有一个ReLU激活函数。

![](/images/Attention Is All You Need !/6.jpg)

尽管线性变换在不同位置上是相同的，但层与层之间使用的是不同的参数。它的另一种描述方式是两个内核大小为1的卷积。输入和输出的维度为dmodel = 512，内部层的维度为dff = 2048。

#### 嵌入和Softmax

与其他序列转化模型相似，我们使用学习到的嵌入将输入词符和输出词符转化为dmodel维的向量。我们还使用普通的线性变换和softmax函数将解码器输出转换为预测的下一个词符的概率。在我们的模型中，两个嵌入层之间和pre-softmax线性变换共享相同的权重矩阵。在嵌入层中，我们将这些权重乘以√dmodel。

#### 位置编码

由于我们的模型不包含RNN和CNN，为了使得模型能够利用序列顺序，我们必须注入序列中关于词符相对或者绝对位置的一些信息。为此，我们将“位置编码”添加到编码器和解码器堆栈底部的输入嵌入中。位置编码和嵌入拥有相同的dmodel维度，所以它们俩可以相加。

我们有多种位置编码可以选择，此工作中，我们使用不同频率的正弦和余弦函数：

![](/images/Attention Is All You Need !/7.jpg)

其中 *pos* 是位置，*i* 是维度。也就是说，位置编码的每个维度对应于一个正弦曲线。这些波长形成一个几何级数，从2π 到10000 ⋅ 2π。我们选择这个函数是因为我们假设它允许模型很容易学习对相对位置的关注，因为对任意确定的偏移k, *PEpos+k*可以表示为PEpos的线性函数。

我们还使用学习到的位置嵌入进行了试验，发现这两个版本产生几乎相同的结果（参见表 3 行(E)）。我们选择了正弦曲线，因为它可以允许模型推断比训练期间遇到的更长的序列。

### 为什么是自注意力机制

本节，我们比较self-attention与循环层和卷积层的各个方面，它们通常用于映射一个可变长度符号序列表示(x1,...,xn) 到另一个等长的序列(z1,...,zn)，其中xi,zi ∈ ℝd，例如一个典型的序列转化编码器或解码器中的隐藏层。我们使用self-attention是考虑到解决三个问题。

一个是每层计算的总复杂度。另一个是可以并行的计算量，以所需的最小序列运算的数量来衡量。

第三个是网络中长距离依赖之间的路径长度。学习长距离依赖性是许多序列转化任务中的关键挑战。影响学习这种依赖性能力的一个关键因素是前向和后向信号必须在网络中传播的路径长度。输入和输出序列中任意位置组合之间的这些路径越短，学习远距离依赖性就越容易。因此，我们还比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。

![](/images/Attention Is All You Need !/8.jpg)

如表1所示，self-attention层通过恒定数量的序列运算将所有位置连接在一起，而RNN层需要O(n) 序列运算。在计算复杂性方面，序列长度n 小于表示维度d 在机器翻译领域较为先进的模型中是最为常见的，例如单词表示法和字节对表示法，如表中所示这种情况下self-attention层比RNN层快。为了提高涉及很长序列的任务的计算性能，可以将self-attention限制在仅考虑大小为r 的邻域。这会将最大路径长度增加到O(n ∕ r)。我们计划在未来的工作中进一步探讨这种方法。

核宽度为k < n的单层卷积不会连接每一对输入和输出的位置。要这么做，在邻近核的情况下需要O(n∕k) 个卷积层， 在扩展卷积的情况下需要O(logk(n)) 个层，它们增加了网络中任意两个位置之间的最长路径的长度。卷积层通常比循环层更昂贵，与因子k有关。然而，可分卷积大幅减少复杂度到O(k ⋅n⋅d + n⋅d2)。然而，即使k = n，一个可分卷积的复杂度等同于self-attention层和point-wise前向层的组合，即我们的模型采用的方法。

间接的好处是self-attention可以产生更具可解释性的模型。我们从我们的模型中研究attention的分布。每个attention head不仅清楚地学习执行不同的任务，许多似乎也展现出了与句子的句法和语义结构相关的行为。
